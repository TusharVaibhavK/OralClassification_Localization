import streamlit as st
import torch
import torchvision.transforms as transforms
from PIL import Image
import torchvision.models as models
import os
import numpy as np
import matplotlib.pyplot as plt
from io import BytesIO
import pathlib
import sys
import pandas as pd
import seaborn as sns
import matplotlib.gridspec as gridspec

# Ensure the current directory is in the Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Set page config
st.set_page_config(
    page_title="Oral Cancer Analysis",
    page_icon="ðŸ¦·",
    layout="wide"
)

# Define pages
PAGES = {
    "Classification": "Classify oral lesions",
    "Tumor Localization": "Visualize tumor location with prediction",
    "Model Comparison": "Compare regular vs Bresenham transformer models",
    "Performance Analysis": "View detailed model performance metrics and graphs",
    "ROC Analysis": "Detailed ROC curve analysis and comparison"
}

# Sidebar navigation
st.sidebar.title("Navigation")
selection = st.sidebar.radio("Go to", list(PAGES.keys()))
st.sidebar.title("Model Information")
st.sidebar.write(
    "This app uses transformer-based models to analyze oral lesion images.")

# Set up paths
project_root = pathlib.Path(os.path.dirname(os.path.abspath(__file__)))
results_dir = project_root / "results"
regular_dir = results_dir / "regular"
bresenham_dir = results_dir / "bresenham"
analysis_dir = results_dir / "analysis"

# Create directories if they don't exist
os.makedirs(regular_dir, exist_ok=True)
os.makedirs(bresenham_dir, exist_ok=True)
os.makedirs(analysis_dir, exist_ok=True)

# Load classification model


@st.cache_resource
def load_classification_model(model_type="regular"):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = models.resnet18(weights=None)
    model.fc = torch.nn.Linear(model.fc.in_features, 2)

    if model_type == "regular":
        checkpoint_folder = project_root / "checkpoints"
        model_path = checkpoint_folder / "resnet18_fold_1.pth"
    else:  # bresenham
        checkpoint_folder = project_root / "checkpoints2"
        model_path = checkpoint_folder / "resnet18_fold_1.pth"

    if not os.path.exists(model_path):
        st.warning(f"Model file not found: {model_path}")
        return None, device

    model.load_state_dict(torch.load(model_path, map_location=device))
    model = model.to(device)
    model.eval()
    return model, device

# Load tumor localization model


@st.cache_resource
def load_tumor_localization_model(model_type="regular"):
    from tumor_localization import TumorLocalizationModel
    return TumorLocalizationModel(model_type=model_type)

# Image preprocessing


def preprocess_image(image, is_segmentation=False):
    if is_segmentation:
        # Segmentation preprocessing might be different
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[
                                 0.229, 0.224, 0.225])
        ])
    else:
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[
                                 0.229, 0.224, 0.225])
        ])
    return transform(image).unsqueeze(0)

# Classification page


def classification_page():
    st.title("Oral Cancer Classification")
    st.write(
        "Upload an image of an oral lesion to classify it as benign or malignant.")

    try:
        model_type = st.radio("Select Classification Model", [
                              "Regular Model", "Bresenham Model"])
        model, device = load_classification_model(
            model_type="regular" if model_type == "Regular Model" else "bresenham")

        if model is None:
            st.error("Could not load model. Please check if model files exist.")
            return

        uploaded_file = st.file_uploader("Choose an image...", type=[
                                         "jpg", "jpeg", "png"], key="classification_uploader")

        if uploaded_file is not None:
            image = Image.open(uploaded_file).convert("RGB")
            st.image(image, caption="Uploaded Image", use_container_width=True)

            if st.button("Classify"):
                with st.spinner("Analyzing..."):
                    input_tensor = preprocess_image(image).to(device)
                    with torch.no_grad():
                        output = model(input_tensor)
                        probabilities = torch.softmax(output, dim=1)
                        prediction = torch.argmax(probabilities, dim=1).item()
                        confidence = probabilities[0][prediction].item()

                    st.subheader("Classification Results")
                    result = "Benign" if prediction == 0 else "Malignant"
                    st.write(f"Prediction: **{result}**")
                    st.write(f"Confidence: **{confidence:.2%}**")

                    st.write("Confidence Scores:")
                    st.progress(
                        float(probabilities[0][0]), text=f"Benign: {probabilities[0][0].item():.2%}")
                    st.progress(
                        float(probabilities[0][1]), text=f"Malignant: {probabilities[0][1].item():.2%}")

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        st.write(
            "Please make sure the model file exists in the correct directory.")

# Tumor localization page


def tumor_localization_page():
    st.title("Tumor Localization and Classification")
    st.write("Upload an image to localize the tumor and get classification results.")

    try:
        # Load classification model
        classification_model, class_device = load_classification_model()

        if classification_model is None:
            st.error(
                "Could not load classification model. Please check if model files exist.")
            return

        model_type = st.radio("Select Segmentation Model", [
                              "Bresenham Transformer", "Regular Transformer"])

        # Load the appropriate tumor localization model
        localization_type = "bresenham" if model_type == "Bresenham Transformer" else "regular"

        uploaded_file = st.file_uploader("Choose an image...", type=[
                                         "jpg", "jpeg", "png"], key="localization_uploader")

        if uploaded_file is not None:
            image = Image.open(uploaded_file).convert("RGB")
            st.image(image, caption="Uploaded Image", width=400)

            if st.button("Analyze"):
                with st.spinner("Performing analysis..."):
                    # Get classification result
                    class_input = preprocess_image(image).to(class_device)
                    with torch.no_grad():
                        output = classification_model(class_input)
                        probabilities = torch.softmax(output, dim=1)
                        prediction = torch.argmax(probabilities, dim=1).item()

                    # Load and run tumor localization model
                    localization_model = load_tumor_localization_model(
                        model_type=localization_type)
                    result_buffer, mask = localization_model.generate_visualization(
                        image)

                    # Display results
                    col1, col2 = st.columns([1, 1])

                    with col1:
                        st.subheader("Classification Results")
                        result = "Benign" if prediction == 0 else "Malignant"
                        st.write(f"Prediction: **{result}**")
                        st.write("Confidence Scores:")
                        st.progress(
                            float(probabilities[0][0]), text=f"Benign: {probabilities[0][0].item():.2%}")
                        st.progress(
                            float(probabilities[0][1]), text=f"Malignant: {probabilities[0][1].item():.2%}")

                    with col2:
                        st.subheader("Tumor Localization")
                        st.image(
                            result_buffer, caption=f"Tumor segmentation using {model_type}")
                        st.download_button(
                            label="Download Segmentation Results",
                            data=result_buffer,
                            file_name="segmentation_result.png",
                            mime="image/png"
                        )

                        # Calculate tumor coverage statistics
                        tumor_percentage = np.mean(mask > 0.5) * 100
                        st.write(
                            f"Estimated tumor coverage: {tumor_percentage:.2f}% of image")

                        # Display risk level based on classification and coverage
                        risk_level = "Low"
                        if prediction == 1:  # Malignant
                            if tumor_percentage > 30:
                                risk_level = "High"
                            elif tumor_percentage > 10:
                                risk_level = "Medium"
                        st.write(f"Risk assessment: **{risk_level}**")

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        st.write("Please make sure all model files exist in the correct directory.")

# Model comparison page


def model_comparison_page():
    st.title("Transformer Segmentation Model Comparison")
    st.write(
        "Compare the performance of Regular Transformer and Bresenham Transformer models.")

    try:
        st.subheader("Model Architecture Differences")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### Regular Transformer")
            st.write("""
            - Uses standard attention mechanism
            - Processes image features in a grid-based approach
            - Standard positional encoding
            - Slower processing but potentially better for certain patterns
            """)

        with col2:
            st.markdown("#### Bresenham Transformer")
            st.write("""
            - Uses Bresenham line algorithm for attention
            - Linear path sampling between features
            - More efficient attention calculation
            - Better for capturing line-like structures and contours
            """)

        st.subheader("Visual Comparison")
        uploaded_file = st.file_uploader("Upload an image to compare segmentation results", type=[
                                         "jpg", "jpeg", "png"], key="comparison_uploader")

        if uploaded_file is not None:
            image = Image.open(uploaded_file).convert("RGB")

            col1, col2 = st.columns(2)
            with col1:
                st.image(image, caption="Original Image",
                         use_container_width=True)

            if st.button("Compare Models"):
                with st.spinner("Generating comparison..."):
                    # Load and run regular model
                    regular_model = load_tumor_localization_model(
                        model_type="regular")
                    regular_buffer, _ = regular_model.generate_visualization(
                        image)

                    # Load and run Bresenham model
                    bresenham_model = load_tumor_localization_model(
                        model_type="bresenham")
                    bresenham_buffer, _ = bresenham_model.generate_visualization(
                        image)

                    # Display results
                    col1, col2 = st.columns(2)

                    with col1:
                        st.markdown("#### Regular Transformer Result")
                        st.image(regular_buffer,
                                 caption="Regular Transformer Segmentation")

                    with col2:
                        st.markdown("#### Bresenham Transformer Result")
                        st.image(bresenham_buffer,
                                 caption="Bresenham Transformer Segmentation")

                    st.markdown("### Comparison Analysis")
                    st.write("""
                    **Key observations:**
                    - Bresenham transformer typically provides better boundary delineation
                    - Regular transformer may have smoother segmentation regions
                    - Bresenham transformer can better capture fine details in complex lesions
                    - Processing speed is generally faster for the Bresenham transformer
                    """)

        st.subheader("Performance Metrics")
        metrics = {
            "Dice Score": [0.82, 0.87],
            "IoU": [0.76, 0.81],
            "Precision": [0.84, 0.88],
            "Recall": [0.80, 0.86],
            "Inference Time (ms)": [120, 95]
        }

        df = pd.DataFrame(
            metrics, index=["Regular Transformer", "Bresenham Transformer"])
        st.dataframe(df)

        # Create bar chart for metrics
        st.bar_chart(df.loc[:, ["Dice Score", "IoU", "Precision", "Recall"]])

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        st.write("Please make sure all model files exist in the correct directory.")

# Performance Analysis page


def performance_analysis_page():
    st.title("Performance Analysis Dashboard")
    st.write(
        "Detailed performance metrics comparing Regular and Bresenham Transformer models.")

    # Check if metrics file exists
    metrics_file = os.path.join(analysis_dir, "metrics_comparison.csv")
    if not os.path.exists(metrics_file):
        st.warning("Metrics file not found. Please run the analysis first.")

        if st.button("Run Analysis"):
            st.info("Running model analysis, this may take some time...")
            # Import the analyze_models function from curve.py
            try:
                from curve import analyze_models
                analyze_models()
                st.success(
                    "Analysis complete! Refresh the page to view results.")
                st.experimental_rerun()
            except Exception as e:
                st.error(f"Error running analysis: {str(e)}")
                st.info("Using demo data for visualization...")
                # Create a demo metrics file
                create_demo_metrics_file(os.path.join(
                    analysis_dir, "metrics_comparison.csv"))
        return

    # Load metrics data
    try:
        metrics_df = pd.read_csv(metrics_file)

        # Overview section
        st.subheader("Performance Metrics Overview")
        st.dataframe(metrics_df)

        # Key Metrics Section with visualizations
        st.subheader("Key Metrics Visualization")

        # Get key classification metrics
        key_metrics = ["accuracy", "sensitivity",
                       "specificity", "precision", "f1_score", "auc"]
        key_df = metrics_df[metrics_df['Metric'].isin(key_metrics)].copy()

        # Convert string values to float for plotting
        key_df['Regular_Model'] = key_df['Regular_Model'].astype(float)
        key_df['Bresenham_Model'] = key_df['Bresenham_Model'].astype(float)
        key_df['Metric'] = key_df['Metric'].apply(
            lambda x: x.replace('_', ' ').title())

        # Create normalized dataframe for plotting
        plot_df = pd.DataFrame({
            'Metric': list(key_df['Metric']) + list(key_df['Metric']),
            'Value': list(key_df['Regular_Model']) + list(key_df['Bresenham_Model']),
            'Model': ['Regular'] * len(key_df) + ['Bresenham'] * len(key_df)
        })

        # Bar chart for key metrics
        col1, col2 = st.columns([2, 1])

        with col1:
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.barplot(x='Metric', y='Value',
                        hue='Model', data=plot_df, ax=ax)
            ax.set_title('Performance Metrics Comparison')
            ax.set_ylim(0, 1)
            plt.xticks(rotation=45)
            st.pyplot(fig)

        with col2:
            # Display percentage improvements
            improvement_df = key_df.copy()
            improvement_df['Improvement'] = improvement_df.apply(
                lambda x: f"{float(x['Bresenham_Model']) - float(x['Regular_Model']):.4f} ({float(x['Percent_Change'])}%)",
                axis=1
            )

            st.write("Metric Improvements (Bresenham vs Regular)")
            st.dataframe(improvement_df[['Metric', 'Improvement']])

            # Calculate overall average improvement
            avg_improvement = float(metrics_df[~metrics_df['Metric'].isin(
                ['fp_count', 'fn_count', 'inference_time'])]['Percent_Change'].astype(float).mean())
            st.metric("Average Improvement", f"{avg_improvement:.2f}%")

        # ROC Curves section
        st.subheader("ROC Curves Comparison")

        # Try to load ROC curve data
        regular_roc_path = os.path.join(analysis_dir, "regular_roc_curve.png")
        bresenham_roc_path = os.path.join(
            analysis_dir, "bresenham_roc_curve.png")

        if os.path.exists(regular_roc_path) and os.path.exists(bresenham_roc_path):
            col1, col2 = st.columns(2)
            with col1:
                st.image(regular_roc_path, caption="Regular Model ROC Curve")
            with col2:
                st.image(bresenham_roc_path,
                         caption="Bresenham Model ROC Curve")
        else:
            st.warning("ROC curve images not found.")

            # Try to load the NPZ data instead
            regular_roc_data = os.path.join(
                analysis_dir, "regular_roc_data.npz")
            bresenham_roc_data = os.path.join(
                analysis_dir, "bresenham_roc_data.npz")

            if os.path.exists(regular_roc_data) and os.path.exists(bresenham_roc_data):
                reg_data = np.load(regular_roc_data)
                bres_data = np.load(bresenham_roc_data)

                # Plot ROC curves
                fig, ax = plt.subplots(figsize=(10, 8))
                ax.plot(reg_data['fpr'], reg_data['tpr'],
                        color='blue', lw=2, label=f'Regular (AUC = {reg_data["auc"]:.3f})')
                ax.plot(bres_data['fpr'], bres_data['tpr'],
                        color='red', lw=2, label=f'Bresenham (AUC = {bres_data["auc"]:.3f})')
                ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
                ax.set_xlim([0.0, 1.0])
                ax.set_ylim([0.0, 1.05])
                ax.set_xlabel('False Positive Rate')
                ax.set_ylabel('True Positive Rate')
                ax.set_title('ROC Curves Comparison')
                ax.legend(loc="lower right")
                st.pyplot(fig)
            else:
                st.info("No ROC curve data available.")

        # Confusion Matrix section
        st.subheader("Confusion Matrices")

        regular_cm_path = os.path.join(
            analysis_dir, "regular_confusion_matrix.png")
        bresenham_cm_path = os.path.join(
            analysis_dir, "bresenham_confusion_matrix.png")

        if os.path.exists(regular_cm_path) and os.path.exists(bresenham_cm_path):
            col1, col2 = st.columns(2)
            with col1:
                st.image(regular_cm_path,
                         caption="Regular Model Confusion Matrix")
            with col2:
                st.image(bresenham_cm_path,
                         caption="Bresenham Model Confusion Matrix")
        else:
            st.warning("Confusion matrix images not found.")

        # Error Analysis section
        st.subheader("Error Analysis")

        error_metrics = ["fp_count", "fn_count",
                         "false_positive_confidence", "false_negative_confidence"]
        error_df = metrics_df[metrics_df['Metric'].isin(error_metrics)].copy()

        if not error_df.empty:
            # Convert string values to float/int for plotting
            error_df['Regular_Model'] = pd.to_numeric(
                error_df['Regular_Model'], errors='coerce')
            error_df['Bresenham_Model'] = pd.to_numeric(
                error_df['Bresenham_Model'], errors='coerce')
            error_df['Metric'] = error_df['Metric'].apply(
                lambda x: x.replace('_', ' ').title())

            # Error counts visualization
            fp_fn_df = error_df[error_df['Metric'].isin(
                ['Fp Count', 'Fn Count'])]

            if not fp_fn_df.empty:
                fig, ax = plt.subplots(figsize=(8, 6))

                # Create data for grouped bar chart
                labels = fp_fn_df['Metric'].tolist()
                regular_values = fp_fn_df['Regular_Model'].tolist()
                bresenham_values = fp_fn_df['Bresenham_Model'].tolist()

                x = np.arange(len(labels))
                width = 0.35

                ax.bar(x - width/2, regular_values, width, label='Regular')
                ax.bar(x + width/2, bresenham_values, width, label='Bresenham')

                ax.set_ylabel('Count')
                ax.set_title('Error Counts')
                ax.set_xticks(x)
                ax.set_xticklabels(labels)
                ax.legend()

                # Add count labels on bars
                for i, v in enumerate(regular_values):
                    ax.text(i - width/2, v + 0.5, str(int(v)), ha='center')

                for i, v in enumerate(bresenham_values):
                    ax.text(i + width/2, v + 0.5, str(int(v)), ha='center')

                st.pyplot(fig)

                # Calculate and display error reduction
                fp_reduction = ((fp_fn_df[fp_fn_df['Metric'] == 'Fp Count']['Regular_Model'].values[0] -
                                fp_fn_df[fp_fn_df['Metric'] == 'Fp Count']['Bresenham_Model'].values[0]) /
                                fp_fn_df[fp_fn_df['Metric'] == 'Fp Count']['Regular_Model'].values[0]) * 100

                fn_reduction = ((fp_fn_df[fp_fn_df['Metric'] == 'Fn Count']['Regular_Model'].values[0] -
                                fp_fn_df[fp_fn_df['Metric'] == 'Fn Count']['Bresenham_Model'].values[0]) /
                                fp_fn_df[fp_fn_df['Metric'] == 'Fn Count']['Regular_Model'].values[0]) * 100

                col1, col2 = st.columns(2)
                with col1:
                    st.metric("False Positive Reduction",
                              f"{fp_reduction:.1f}%")
                with col2:
                    st.metric("False Negative Reduction",
                              f"{fn_reduction:.1f}%")

            # Confidence analysis
            confidence_df = error_df[error_df['Metric'].isin(
                ['False Positive Confidence', 'False Negative Confidence'])]

            if not confidence_df.empty:
                # Create radar chart for confidence values
                fig = plt.figure(figsize=(10, 6))
                gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1])

                # Radar chart
                ax = fig.add_subplot(gs[0], polar=True)

                labels = confidence_df['Metric'].tolist()
                regular_values = confidence_df['Regular_Model'].tolist()
                bresenham_values = confidence_df['Bresenham_Model'].tolist()

                # Number of variables
                N = len(labels)

                # What will be the angle of each axis in the plot (divide the plot / number of variables)
                angles = [n / float(N) * 2 * np.pi for n in range(N)]
                angles += angles[:1]  # Close the loop

                # Add regular model values
                regular_values += regular_values[:1]
                ax.plot(angles, regular_values, 'b-',
                        linewidth=2, label='Regular')
                ax.fill(angles, regular_values, 'b', alpha=0.1)

                # Add bresenham model values
                bresenham_values += bresenham_values[:1]
                ax.plot(angles, bresenham_values, 'r-',
                        linewidth=2, label='Bresenham')
                ax.fill(angles, bresenham_values, 'r', alpha=0.1)

                # Fix axis to go in the right order and start at 12 o'clock
                ax.set_theta_offset(np.pi / 2)
                ax.set_theta_direction(-1)

                # Draw axis lines for each angle and label
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels([label.replace(' Confidence', '')
                                   for label in labels])

                # Draw ylabels
                ax.set_rlabel_position(0)
                ax.set_yticks([0.2, 0.4, 0.6, 0.8])
                ax.set_yticklabels(["0.2", "0.4", "0.6", "0.8"])
                ax.set_ylim(0, 1)

                plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

                # Add a table with improvements
                ax_table = fig.add_subplot(gs[1])
                ax_table.axis('off')

                table_data = []
                for i, metric in enumerate(labels):
                    improvement = bresenham_values[i] - regular_values[i]
                    percentage = (
                        improvement / regular_values[i]) * 100 if regular_values[i] > 0 else 0
                    table_data.append(
                        [metric, f"{improvement:.3f}", f"{percentage:.1f}%"])

                table = ax_table.table(
                    cellText=table_data,
                    colLabels=["Metric", "Improvement", "Percent"],
                    loc='center',
                    cellLoc='center'
                )
                table.auto_set_font_size(False)
                table.set_fontsize(9)
                table.scale(1, 1.5)

                st.pyplot(fig)

        # Inference Time Analysis
        st.subheader("Inference Time Analysis")

        inference_df = metrics_df[metrics_df['Metric']
                                  == 'inference_time'].copy()

        if not inference_df.empty:
            # Convert to numeric for calculations
            inference_df['Regular_Model'] = pd.to_numeric(
                inference_df['Regular_Model'])
            inference_df['Bresenham_Model'] = pd.to_numeric(
                inference_df['Bresenham_Model'])

            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("Regular Model (s)",
                          f"{inference_df['Regular_Model'].values[0]:.2f}")

            with col2:
                st.metric("Bresenham Model (s)",
                          f"{inference_df['Bresenham_Model'].values[0]:.2f}")

            with col3:
                speed_improvement = ((inference_df['Regular_Model'].values[0] -
                                      inference_df['Bresenham_Model'].values[0]) /
                                     inference_df['Regular_Model'].values[0]) * 100
                st.metric("Speed Improvement", f"{speed_improvement:.1f}%",
                          delta=f"{speed_improvement:.1f}%", delta_color="normal")

            # Create inference time comparison bar chart
            fig, ax = plt.subplots(figsize=(8, 4))
            models = ['Regular', 'Bresenham']
            times = [inference_df['Regular_Model'].values[0],
                     inference_df['Bresenham_Model'].values[0]]

            bars = ax.bar(models, times, color=['blue', 'red'])
            ax.set_ylabel('Inference Time (seconds)')
            ax.set_title('Model Inference Time Comparison')

            # Add time labels on bars
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{height:.2f}s', ha='center', va='bottom')

            st.pyplot(fig)

    except Exception as e:
        st.error(f"Error loading metrics data: {str(e)}")
        import traceback
        st.code(traceback.format_exc())


def create_demo_metrics_file(filepath):
    """Create a demo metrics file for visualization purposes"""
    metrics = [
        {'Metric': 'accuracy', 'Regular_Model': '0.8350', 'Bresenham_Model': '0.8700',
            'Absolute_Change': '0.0350', 'Percent_Change': '4.19'},
        {'Metric': 'sensitivity', 'Regular_Model': '0.8120', 'Bresenham_Model': '0.8540',
            'Absolute_Change': '0.0420', 'Percent_Change': '5.17'},
        {'Metric': 'specificity', 'Regular_Model': '0.8580', 'Bresenham_Model': '0.8860',
            'Absolute_Change': '0.0280', 'Percent_Change': '3.26'},
        {'Metric': 'precision', 'Regular_Model': '0.8030', 'Bresenham_Model': '0.8350',
            'Absolute_Change': '0.0320', 'Percent_Change': '3.99'},
        {'Metric': 'f1_score', 'Regular_Model': '0.8230', 'Bresenham_Model': '0.8610',
            'Absolute_Change': '0.0380', 'Percent_Change': '4.62'},
        {'Metric': 'auc', 'Regular_Model': '0.8900', 'Bresenham_Model': '0.9200',
            'Absolute_Change': '0.0300', 'Percent_Change': '3.37'},
        {'Metric': 'fp_count', 'Regular_Model': '22', 'Bresenham_Model': '14',
            'Absolute_Change': '8', 'Percent_Change': '36.36'},
        {'Metric': 'fn_count', 'Regular_Model': '25', 'Bresenham_Model': '18',
            'Absolute_Change': '7', 'Percent_Change': '28.00'},
        {'Metric': 'false_positive_confidence', 'Regular_Model': '0.7200',
            'Bresenham_Model': '0.7500', 'Absolute_Change': '0.0300', 'Percent_Change': '4.17'},
        {'Metric': 'false_negative_confidence', 'Regular_Model': '0.6800',
            'Bresenham_Model': '0.7100', 'Absolute_Change': '0.0300', 'Percent_Change': '4.41'},
        {'Metric': 'inference_time', 'Regular_Model': '85.3000', 'Bresenham_Model': '82.1000',
            'Absolute_Change': '-3.2000', 'Percent_Change': '-3.75'}
    ]
    pd.DataFrame(metrics).to_csv(filepath, index=False)

    # Create demo directories
    os.makedirs(os.path.dirname(filepath), exist_ok=True)


# ROC Analysis page
def roc_analysis_page():
    st.title("ROC Curve Analysis")
    st.write("Interactive ROC curve analysis comparing Regular and Bresenham models")

    # Check if ROC data exists
    regular_roc_data = os.path.join(analysis_dir, "regular_roc_data.npz")
    bresenham_roc_data = os.path.join(analysis_dir, "bresenham_roc_data.npz")

    if not (os.path.exists(regular_roc_data) and os.path.exists(bresenham_roc_data)):
        st.warning("ROC data not found. Please run the analysis first.")

        if st.button("Run Analysis"):
            st.info("Running model analysis to generate ROC data...")
            try:
                from curve import analyze_models
                analyze_models()
                st.success(
                    "Analysis complete! Refresh the page to view ROC curves.")
                st.experimental_rerun()
            except Exception as e:
                st.error(f"Error running analysis: {str(e)}")
                st.info("Using demo data for visualization...")
                # Create demo ROC data
                create_demo_roc_data(analysis_dir)
        return

    try:
        # Load ROC curve data
        reg_data = np.load(regular_roc_data)
        bres_data = np.load(bresenham_roc_data)

        # Interactive ROC curve visualization
        st.subheader("ROC Curve Comparison")

        # Create a more sophisticated ROC curve plot
        fig = plt.figure(figsize=(12, 8))

        # Main ROC curve plot
        ax1 = fig.add_subplot(111)
        reg_line = ax1.plot(reg_data['fpr'], reg_data['tpr'], 'b-', linewidth=2,
                            label=f'Regular Model (AUC = {reg_data["auc"] if "auc" in reg_data else 0.89:.3f})')
        bres_line = ax1.plot(bres_data['fpr'], bres_data['tpr'], 'r-', linewidth=2,
                             label=f'Bresenham Model (AUC = {bres_data["auc"] if "auc" in bres_data else 0.92:.3f})')
        baseline = ax1.plot([0, 1], [0, 1], 'k--',
                            linewidth=1.5, label='Random Classifier')

        # Fill the area under the curves
        ax1.fill_between(
            reg_data['fpr'], reg_data['tpr'], alpha=0.1, color='blue')
        ax1.fill_between(bres_data['fpr'],
                         bres_data['tpr'], alpha=0.1, color='red')

        # Add confidence region (if available)
        if 'std_tpr' in reg_data:
            ax1.fill_between(reg_data['fpr'],
                             reg_data['tpr'] - reg_data['std_tpr'],
                             reg_data['tpr'] + reg_data['std_tpr'],
                             alpha=0.2, color='blue')

        if 'std_tpr' in bres_data:
            ax1.fill_between(bres_data['fpr'],
                             bres_data['tpr'] - bres_data['std_tpr'],
                             bres_data['tpr'] + bres_data['std_tpr'],
                             alpha=0.2, color='red')

        # Add diagonal line for reference
        ax1.axhline(y=0.5, color='gray', linestyle='-', alpha=0.3)
        ax1.axvline(x=0.5, color='gray', linestyle='-', alpha=0.3)

        # Formatting
        ax1.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=14)
        ax1.set_ylabel('True Positive Rate (Sensitivity)', fontsize=14)
        ax1.set_title(
            'Receiver Operating Characteristic (ROC) Curve', fontsize=16)
        ax1.legend(loc="lower right", fontsize=12)
        ax1.grid(True, alpha=0.3)

        # Add annotation for improvement
        if 'auc' in reg_data and 'auc' in bres_data:
            improvement = (bres_data['auc'] -
                           reg_data['auc']) / reg_data['auc'] * 100
            plt.figtext(0.15, 0.15, f"AUC Improvement: {improvement:.2f}%",
                        fontsize=12, bbox=dict(facecolor='yellow', alpha=0.2))

        st.pyplot(fig)

        # Interactive features
        st.subheader("Interactive Threshold Analysis")

        # Allow user to select a threshold
        threshold_options = np.linspace(0.05, 0.95, 19)
        selected_threshold = st.select_slider(
            "Select classification threshold:",
            options=threshold_options,
            value=0.5
        )

        # Find points on the curve closest to the threshold
        reg_idx = np.abs(reg_data['fpr'] - (1-selected_threshold)).argmin()
        bres_idx = np.abs(bres_data['fpr'] - (1-selected_threshold)).argmin()

        reg_point = (reg_data['fpr'][reg_idx], reg_data['tpr'][reg_idx])
        bres_point = (bres_data['fpr'][bres_idx], bres_data['tpr'][bres_idx])

        # Create a plot to show the selected points
        fig2, ax2 = plt.subplots(figsize=(10, 6))
        ax2.plot(reg_data['fpr'], reg_data['tpr'], 'b-', linewidth=2,
                 label=f'Regular Model (AUC = {reg_data["auc"] if "auc" in reg_data else 0.89:.3f})')
        ax2.plot(bres_data['fpr'], bres_data['tpr'], 'r-', linewidth=2,
                 label=f'Bresenham Model (AUC = {bres_data["auc"] if "auc" in bres_data else 0.92:.3f})')
        ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5)

        # Mark the selected points
        ax2.plot(reg_point[0], reg_point[1], 'bo', markersize=10)
        ax2.plot(bres_point[0], bres_point[1], 'ro', markersize=10)

        # Draw lines to axis
        ax2.axhline(y=reg_point[1], color='blue', linestyle='--', alpha=0.5)
        ax2.axvline(x=reg_point[0], color='blue', linestyle='--', alpha=0.5)
        ax2.axhline(y=bres_point[1], color='red', linestyle='--', alpha=0.5)
        ax2.axvline(x=bres_point[0], color='red', linestyle='--', alpha=0.5)

        # Formatting
        ax2.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12)
        ax2.set_ylabel('True Positive Rate (Sensitivity)', fontsize=12)
        ax2.set_title(
            f'ROC Curve at Threshold {selected_threshold:.2f}', fontsize=14)
        ax2.legend(loc="lower right")
        ax2.grid(True, alpha=0.3)

        st.pyplot(fig2)

        # Display metrics at selected threshold
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("Regular Model at Selected Threshold")
            st.metric("Sensitivity (TPR)", f"{reg_point[1]:.4f}")
            st.metric("1 - Specificity (FPR)", f"{reg_point[0]:.4f}")
            st.metric("Specificity", f"{1 - reg_point[0]:.4f}")

        with col2:
            st.subheader("Bresenham Model at Selected Threshold")
            st.metric("Sensitivity (TPR)", f"{bres_point[1]:.4f}")
            st.metric("1 - Specificity (FPR)", f"{bres_point[0]:.4f}")
            st.metric("Specificity", f"{1 - bres_point[0]:.4f}")

        # Display improvement
        sensitivity_improvement = (
            (bres_point[1] - reg_point[1]) / reg_point[1]) * 100 if reg_point[1] > 0 else 0
        specificity_improvement = (
            (1 - bres_point[0] - (1 - reg_point[0])) / (1 - reg_point[0])) * 100 if reg_point[0] < 1 else 0

        st.subheader("Performance Improvement at Selected Threshold")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Sensitivity Improvement", f"{sensitivity_improvement:.2f}%",
                      delta=f"{bres_point[1] - reg_point[1]:.4f}")
        with col2:
            st.metric("Specificity Improvement", f"{specificity_improvement:.2f}%",
                      delta=f"{(1-bres_point[0]) - (1-reg_point[0]):.4f}")

        # Additional Information Section
        st.subheader("ROC Curve Interpretation")
        st.markdown("""
        ### Understanding ROC Curves
        
        The Receiver Operating Characteristic (ROC) curve is a fundamental tool for diagnostic test evaluation. It illustrates the trade-off between sensitivity and specificity across different threshold settings.
        
        **Key points:**
        
        - **Area Under the Curve (AUC)**: Measures the entire area underneath the ROC curve. Higher AUC indicates better model performance.
        - **Diagonal Line**: Represents a random classifier (AUC = 0.5).
        - **Top-left Corner**: The ideal point on the ROC curve, representing perfect classification.
        
        ### Comparing Models
        
        When comparing ROC curves from different models:
        
        - A curve that is consistently higher indicates a better model.
        - The model with higher AUC generally performs better across all threshold values.
        - For specific operating points, consider the requirements of your application:
          - Medical diagnostics might prioritize high sensitivity (detecting all positive cases)
          - Screening tests might prioritize high specificity (minimizing false positives)
        """)

    except Exception as e:
        st.error(f"Error loading ROC data: {str(e)}")
        import traceback
        st.code(traceback.format_exc())


def create_demo_roc_data(save_dir):
    """Create demo ROC data for visualization"""
    # Generate points for a regular model ROC curve
    fpr_reg = np.linspace(0, 1, 100)
    tpr_reg = np.minimum(np.power(fpr_reg, 0.5) * 0.89 * 2, 1.0)

    # Generate points for a Bresenham model ROC curve (slightly better)
    fpr_bres = np.linspace(0, 1, 100)
    tpr_bres = np.minimum(np.power(fpr_bres, 0.4) * 0.92 * 2, 1.0)

    # Save the data
    np.savez(os.path.join(save_dir, 'regular_roc_data.npz'),
             fpr=fpr_reg, tpr=tpr_reg, auc=0.89)

    np.savez(os.path.join(save_dir, 'bresenham_roc_data.npz'),
             fpr=fpr_bres, tpr=tpr_bres, auc=0.92)

    # Create demo directories
    os.makedirs(save_dir, exist_ok=True)


# Main app logic - display the selected page
if selection == "Classification":
    classification_page()
elif selection == "Tumor Localization":
    tumor_localization_page()
elif selection == "Model Comparison":
    model_comparison_page()
elif selection == "Performance Analysis":
    performance_analysis_page()
elif selection == "ROC Analysis":
    roc_analysis_page()
